{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine(SVM)\n",
    "\n",
    "- Author: Guo Zhang\n",
    "- Created: 2017-03-22\n",
    "- Updated: 2017-03-26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Classification\n",
    "- Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Classification(SVC)\n",
    "### Principles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Start point:  Seperating  Hyperplane\n",
    "\n",
    "Let's begin with the simplest two classes problems. Support we want to classify **two** kinds of fruit, apple and banana. Support we have selected the features(such as color, weight, volume, etc.) of apples and banana into a n-dimensional space. The most natural way to divide the two class is to cut the space with a piece of paper. The boundary is the \"paper\" cutting the space. It is a line in 2-dimensional space, a plane in 3-dimensional space, and a **hyperplane** in n-dimensional space.  \n",
    "\n",
    "\n",
    "#### Maximal Margin Classifier: Optimal Sperating Hyperplane\n",
    "\n",
    "\n",
    "However, there are infinite<!--to be sure--> many margins. Of course, we want to find a **optimal seperate hyperplane**. We define the mimimal distance between a given obseration in the class and the given seperating hyperplane. Therefore, we define **maximal margin hyperplane**(optimal seperate hyperplane) as the seperating hyperplane that maximize the margin.\n",
    "\n",
    "\n",
    "![Hard Margin](SVM_hard.jpg)\n",
    "\n",
    "\n",
    "The observations (expressed by vectors in the linear space) support the margin are defined as **support vectors**. These \"vectors\" \"support\" the \"machine\", that's how **support vector machine** was named.\n",
    "\n",
    "\n",
    "#### From Hard Margin to Soft Margin\n",
    "\n",
    "Maximal margin classifier mentioned above leaves us two problems. First, we can use it only one can find seperating hyperplanes. In many cases, there exists no seperating hyperplanes. Second, the optimal seperating hyperplanes will be sensitive to the change of support vectors. In other words, the classifier is not robust. Therefore, we hope to improve the classifier so that it can tolerate some outliers. It makes the introduction of **soft margin**. It allows some incorrect ones classified into the other sides.\n",
    "\n",
    "\n",
    "![Soft Margin](SVM_soft.jpg)\n",
    "\n",
    "\n",
    "#### From Linear to Nonlinear\n",
    "\n",
    "Soft margin classifier gives us an opportunity to avoid robustness problems. However, we do still not completely solve the problem if there is no linear seperating hypreplane. Therefore, we need to look for methods to construct nonlinear hyperplanes. The key of constructing nonlinear soft margin is to select proper **kernel** function instead inner product for linear kernel function.\n",
    "\n",
    "![Nonlinear](SVM_nonlinear.jpg)\n",
    "\n",
    "\n",
    "#### From Two Classes to Multiple Classes\n",
    "\n",
    "Now let's consider multiple classes problems. Suppose now we need to classify three kinds of fruit, apple, banana and pear.\n",
    "<!--not understand the different methods-->\n",
    "\n",
    "![Multiple Class](SVM_mul.jpg)\n",
    "\n",
    "\n",
    "#### One-Class Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Seperating Hyperplane\n",
    "<!--finished-->\n",
    "\n",
    "A **hyperplane** in a n-dimensional space is defined as\n",
    "\n",
    "$$\n",
    "\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... +\\beta_n X_n = 0\n",
    "$$\n",
    "\n",
    "Then, any $X=(X_1,X_2,...,X_n)^T$ in the space, if it is not on the hyperplane, it will satisfy\n",
    "\n",
    "$$\n",
    "\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... +\\beta_n X_n > 0\n",
    "$$\n",
    "\n",
    "or\n",
    "$$\n",
    "\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... +\\beta_n X_n < 0\n",
    "$$\n",
    "\n",
    "Therefore, if we want to classifiy a set of $m$ training data with two classes, we can try to find such **seperating hyperplane** *if it exists*. If we label two classes as {1,-1}, the classifier should have this property without loss of generality:\n",
    "\n",
    "$$\n",
    "\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + ... +\\beta_n x_{in} > 0 \\quad if \\quad y_i=1\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + ... +\\beta_n x_{in} < 0  \\quad if \\quad y_i=-1\n",
    "$$\n",
    "\n",
    "Equivalently, for all $i=1,2,...,m$\n",
    "\n",
    "$$\n",
    "y_i(\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + ... +\\beta_n x_{in}) > 0 \n",
    "$$\n",
    "\n",
    "Therefore, if seperating hyperplane exists, we can define $f(x)=\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + ... +\\beta_n x_{in}$. If f(x) is positive, it is in class 1; if it is negative, it is in class -1.\n",
    "\n",
    "What's more, when f(x) is far away from 0, the point is far from seperating hyperplane; when f(x) is close to 0, the point is close to the seperating hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maximal Margin Classifier \n",
    "<!--finished-->\n",
    "\n",
    "If seperating hyperplane exists, there exists infinite number of seperating hyperplane. There comes a natural problem: what is the **optimal seperating hyperplane** that classifies two classes of training data best?\n",
    "\n",
    "A natural way to think is that we want this seperating hyperplane as far as it can from two classes. In the end of last part, we propose that when f(x) will be far away from 0 if the observation is far from seperating hyperplane. We should clearly define what is \"far\". The distance from different observations to a given seperating hyperplane is different. We choose the minimal distance, and define it as **margin**. Now we change seperating hyperplane, than the margin will change together. Naturally, we can define the optimal seperating hyperplane as the one with maximal margin. This kind of classifier is called **maximal margin classifier**. And the observations that support the margin are defined as **support vector** (The observations are expressed as vectors in a linear space). This is how the **support vector machine** is named.\n",
    "\n",
    "Mathematically, we construct maximal margin classifier as:\n",
    "\n",
    "$$\\max_{\\beta_0, \\beta_1, \\beta_2,\\ldots, \\beta_p} M$$\n",
    "\n",
    "subject to \n",
    "\n",
    "$$\\sum_{j=1}^p \\beta_j^2=1$$\n",
    "$$y_i(\\beta_0+\\beta_1 x_{i1}+\\ldots+\\beta_p x_{ip})\\geq M$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Soft Margin Classifier\n",
    "\n",
    "Maximal margin classifier defined in the last section is also called **hard margin classifier** because we don't give the classifier an opportunity to adjust. From the definition of maximal margin classifier, we can find two problems:\n",
    "\n",
    "1. the maximal margin classifier is very sensitive to the support vectors. If we change another dataset, the support vectors will change. Therefore, the classifier is not robust.\n",
    "2. some of observations(outliers) in one class are mixed into another class. So the seperating hyperplane does not exist.\n",
    "\n",
    "Therefore, we need to allow some adjustment for the hard margin classifier. There comes the notion of **soft margin classifier**.\n",
    "\n",
    "\n",
    "Mathematically, the soft margin classifier can be constructed as:\n",
    "\n",
    "$$\\max_{\\beta_0, \\beta_1, \\beta_2,\\ldots, \\beta_p, \\varepsilon_1, \\ldots, \\varepsilon_n} M$$\n",
    "\n",
    "subject to \n",
    "\n",
    "$$\\sum_{j=1}^p \\beta_j^2=1$$\n",
    "$$y_i(\\beta_0+\\beta_1 x_{i1}+\\ldots+\\beta_p x_{ip})\\geq M(1-\\varepsilon_i)$$\n",
    "$$\\varepsilon_i \\geq 0, \\sum_{i=1}^{n}\\varepsilon_i \\leq C$$\n",
    "\n",
    "The difference is that we allow some **slack variables**, labeled by $\\varepsilon_i$, to allow some outliers to be classified into the wrong side. **$C$** is a nonnegative **tuning parameter**, defined as the summation of all slack variables. Therefore, we can understand $C$ as the budget of violations we are going to tolerate. Technologically, $C$ controls the bias-variance trade-off. The larger $C$, the larger violations (larger variance) we will tolerate; the smaller $C$, the smaller violations (smaller variacnce). If $C=0$, it will degenerate to hard margin. So soft margin can be considered as a more general case.\n",
    "\n",
    "What's more, the construction also shows that support vector classifier is robust to points far away from the seperatng hyperplane, which is similar with LDA but different with logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nonlinear Support Vector Classifier\n",
    "\n",
    "Until now, we have known how to classify data with linear boundary. However, there are no linear boundary in many cases in practices. So the problem of no linear seperating hyperplane is not solved completely. Therefore, we need to find a method to construct nonlinear boundary. \n",
    "\n",
    "Let's look back at the linear classifier first. The linear classifier is \n",
    "\n",
    "$$\n",
    "\n",
    "$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple-Classes Support Vector Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-Class Support Vector Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Implements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import SVM classes\n",
    "from sklearn.svm import SVC, LinearSVC, OneClassSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare data\n",
    "# data for two classes\n",
    "x_two = [[0, 0], [2, 2]]\n",
    "y_two = [1, 2]\n",
    "\n",
    "x_two_predict = [[1, 2]]\n",
    "\n",
    "# data for multiple classes\n",
    "x_mul = [[0], [1], [2], [3]]\n",
    "y_mul = [0, 1, 2, 3]\n",
    "\n",
    "x_mul_predict = [[2.4]]\n",
    "\n",
    "# data for one class\n",
    "x_one = [[0],[2],[4],[5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Soft Margin\n",
    "There are two implements by *sklearn*.\n",
    "- Two classes linear models by SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SVC(kernel='linear')\n",
    "clf.fit(x_two, y_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(x_two_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Two classes linear models by LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LinearSVC()\n",
    "clf.fit(x_two, y_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(x_two_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nonlinear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import SVC class\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SVC()\n",
    "clf.fit(x_two, y_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(x_two_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Muti-Class SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Linear multiple classes models by SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SVC(kernel='linear')\n",
    "clf.fit(x_mul, y_mul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(x_mul_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Linear multiple classes models by LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LinearSVC()\n",
    "clf.fit(x_mul, y_mul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(x_mul_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the result of two implements are not the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nonlinear multiple classes models by SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SVC()\n",
    "clf.fit(x_mul, y_mul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(x_mul_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-Class SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = OneClassSVM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma='auto', kernel='rbf',\n",
       "      max_iter=-1, nu=0.5, random_state=None, shrinking=True, tol=0.001,\n",
       "      verbose=False)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(x_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1., -1., -1., -1.,  1., -1.,  1.])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict([[5],[3.55],[2],[3],[0],[-1],[4.3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that it will return **1** of it is in the class, and return **-1** if not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Economic Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.researchgate.net/profile/Young-Chan_Lee/publication/222580945_Bankruptcy_prediction_using_support_vector_machine_with_optimal_choice_of_kernel_function_parameters/links/02e7e52bac4f202c28000000/Bankruptcy-prediction-using-support-vector-machine-with-optimal-choice-of-kernel-function-parameters.pdf\n",
    "- http://svms.org/finance/HuangNakamoriWang2005.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Regression(SVR)\n",
    "### Principles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Implements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import SVR class\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# simulated data\n",
    "x = [[0, 0], [2, 2]]\n",
    "y = [1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
       "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit \n",
    "clf = SVR()\n",
    "clf.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.71369217])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prediction\n",
    "clf.predict([[1, 2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing to previous results, notice that the prediction of support vector **regression** is different from support vector **classification**. Regression is continous while classification is discrete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Economic Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vapnik(1995)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Chapter 9, [An Introduction to Statistical Learning with R](http://www-bcf.usc.edu/~gareth/ISL/)\n",
    "- Chapter 12, [Elements of Statistical Learning](http://statweb.stanford.edu/~tibs/ElemStatLearn/)\n",
    "- Chapter 6, [Machine Learning in Action](https://github.com/pbharrin/machinelearninginaction)\n",
    "- [A Tutorial to Support Vector Regression](http://alex.smola.org/papers/2003/SmoSch03b.pdf)\n",
    "- [Support Vector Regression Machine](http://papers.nips.cc/paper/1238-support-vector-regression-machines.pdf)\n",
    "- [Support Vector Machines - scikit-learn](http://scikit-learn.org/stable/modules/svm.html#svm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
